% ICECET / IEEE Conference Paper (A4) — Expanded Version (Target: ~5 pages)
% Notes:
% 1) Replace placeholders marked with <<<...>>>.
% 2) Ensure figures exist in your project folder (or comment them out).
% 3) Verify that reported numbers correspond to your exact split/protocol.

\documentclass[conference,a4paper]{IEEEtran}

% -------------------- Packages (IEEE-safe set) --------------------
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{siunitx}

% Algorithm (template already includes algorithmic; keep it minimal)
\usepackage{algorithmic}

% Subfigures (IEEE recommendation: caption=false)
\usepackage[caption=false,font=footnotesize]{subfig}

\usepackage{hyperref}
\hypersetup{
  hidelinks=true,
  pdfauthor={Can Ke\c{c}ilio\u{g}lu},
  pdftitle={Handwritten Character Recognition: Classical Baselines to Deep Ensembles}
}

% -------------------- Title / Authors --------------------
\title{Handwritten Character Recognition: A Reproducible Comparison from HOG+XGBoost to Optimized CNNs and Ensemble Inference}

\author{
\IEEEauthorblockN{Can Ke\c{c}ilio\u{g}lu}
\IEEEauthorblockA{
Department of Mechatronics Engineering\\
Turkish-German Universtiy, Istanbul, Turkiye\\
Email: cankecilioglu@gmail.com
}
}

\begin{document}
\maketitle

% -------------------- Abstract / Keywords --------------------
\begin{abstract}
Handwritten character recognition (HCR) remains challenging due to large intra-class variability and strong inter-class similarity, especially for case-sensitive pairs (e.g., \textit{w/W}, \textit{s/S}) and visually confusable symbols (e.g., \textit{0/O}, \textit{l/I}).
This paper presents a structured, reproducible comparison of (i) a classical feature-engineering baseline using Histogram of Oriented Gradients (HOG) with gradient-boosted decision trees (XGBoost), (ii) an optimized VGG-style convolutional neural network (CNN), and (iii) a multi-model ensemble combined with test-time augmentation (TTA).
On a 62-class handwritten character dataset (digits, uppercase, lowercase), HOG+XGBoost yields \SI{68.11}{\percent} accuracy, the best single CNN reaches \SI{89.45}{\percent}, and ensemble+TTA achieves \SI{93.16}{\percent} under a fixed evaluation protocol.
Beyond aggregate metrics, error analysis indicates residual errors concentrate on case-sensitive confusions rather than structural failures, suggesting isolated-character HCR can be limited by missing contextual cues.
\end{abstract}

\begin{IEEEkeywords}
handwritten character recognition, HOG, XGBoost, convolutional neural networks, hyperparameter optimization, ensemble learning, test-time augmentation, error analysis
\end{IEEEkeywords}

% -------------------- 1. Introduction --------------------
\section{Introduction}
Handwritten character recognition (HCR) is a core component of document digitization pipelines, form processing, and educational technologies.
Unlike printed OCR, HCR exhibits substantial morphological variation across writers and styles, while many classes are intrinsically ambiguous when viewed in isolation (e.g., \textit{0} vs. \textit{O}, \textit{l} vs. \textit{I}, and multiple case variants).
As a result, classical feature-engineering pipelines often offer attractive compute efficiency but may saturate earlier than deep models, especially when fine-grained distinctions are required.

In this work, we explicitly frame the problem as a \textbf{62-way isolated-character classification} task without language context.
This constraint is important: several remaining errors cannot be resolved reliably from a single glyph alone, and thus represent a practical upper bound for purely visual models on such datasets.

This paper contributes:
\begin{itemize}
  \item A \textbf{reproducible evaluation protocol} (fixed split strategy, seeds, and metrics) enabling fair comparison across model families.
  \item A \textbf{strong classical baseline} (HOG+XGBoost) and an \textbf{optimized CNN} (VGG-style) with documented training decisions.
  \item An \textbf{ensemble inference pipeline} combining diverse models and \textbf{test-time augmentation} (TTA), with a clear cost/benefit discussion.
  \item \textbf{Error characterization} focusing on confusion clusters and case sensitivity, clarifying what improvements are likely (and unlikely) without context.
\end{itemize}

% -------------------- 2. Related Work --------------------
\section{Related Work}
Gradient-based descriptors such as HOG have long been used for recognition tasks by capturing local orientation statistics \cite{dalal2005hog}.
For classification, boosted decision trees and XGBoost are widely adopted due to strong performance on structured features and practical training speed \cite{chen2016xgboost}.
Deep CNNs, particularly VGG-style designs, provide hierarchical feature learning that often surpasses hand-crafted features in vision tasks \cite{simonyan2014vgg}.
Recent improvements in deep training include smoother activations such as Swish \cite{ramachandran2017swish} and learning-rate schedules (e.g., cosine decay) \cite{loshchilov2016sgdr}, which can improve convergence.
For boosting inference performance, ensembles and TTA are common techniques to reduce prediction variance, albeit at increased compute cost.
Finally, calibration and confidence estimation are increasingly used to manage ambiguous inputs in classification systems \cite{guo2017calibration}.

% -------------------- 3. Dataset and Preprocessing --------------------
\section{Dataset and Preprocessing}
\subsection{Dataset}
We evaluate on a labeled dataset of handwritten characters with $C=62$ classes: digits (0--9), uppercase letters (A--Z), and lowercase letters (a--z), comprising $N=3410$ images.
Each image is converted to grayscale and resized to $64\times64$ pixels.

\subsection{Train/Validation/Test Protocol}
To ensure reproducibility and prevent optimistic reporting, all results are reported using a \textbf{fixed stratified split}:
\begin{itemize}
  \item Train: \SI{70}{\percent}, Validation: \SI{15}{\percent}, Test: \SI{15}{\percent}
  \item Stratification by class to preserve label proportions
  \item Random seed: \texttt{<<<SEED>>>} (e.g., 42)
\end{itemize}
\textbf{Important:} If \SI{93.16}{\percent} was obtained under a different split (or cross-validation), update this subsection and keep it consistent across all models.

\subsection{Normalization and Augmentation}
Pixel values are normalized to $[0,1]$.
Training uses on-the-fly augmentation to improve robustness:
\begin{itemize}
  \item Rotation: $\pm 15^\circ$
  \item Translation: up to \SI{15}{\percent} of image width/height
  \item Zoom: up to \SI{20}{\percent}
\end{itemize}
Augmentations are applied to training only; validation and test remain unaugmented except where TTA is explicitly enabled at inference.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{augmentation_samples.png}
  \caption{Representative training-time augmentations (rotation/shift/zoom) applied to the resized $64\times64$ grayscale inputs.}
  \label{fig:augmentation}
\end{figure}

% -------------------- 4. Methods --------------------
\section{Methods}

\subsection{Classical Baseline: HOG + XGBoost}
Given the limited dataset size, we first evaluate a classical pipeline.
HOG features are extracted using $8\times8$ cells and $8$ orientation bins, producing a $1568$-dimensional descriptor per image (replacing raw $4096$ pixel inputs).
An XGBoost classifier is trained with tuned parameters (e.g., \texttt{n\_estimators}, \texttt{max\_depth}, \texttt{learning\_rate}), selected on validation accuracy.
The motivation is to establish a compute-efficient reference point and quantify how far handcrafted features can go in a 62-class setting.

\subsection{Deep Model Family: VGG-Style CNN}
We employ a VGG-style CNN consisting of repeated convolutional blocks (Conv--BN--Activation) followed by max pooling, then dense layers for classification.
This design is intentionally simple and interpretable: it allows controlled experimentation with activation functions, regularization, and schedules without architectural confounds.

\subsubsection{Architecture Summary}
Table~\ref{tab:arch} provides a compact description of the best-performing single-model configuration.
Exact channel widths and dropout rates are selected via Bayesian tuning on the validation set.

\begin{table}[t]
\centering
\caption{Optimized VGG-Style CNN (Best Single Model) — High-Level Specification}
\label{tab:arch}
\begin{tabular}{@{}p{0.30\linewidth}p{0.62\linewidth}@{}}
\toprule
\textbf{Stage} & \textbf{Details} \\
\midrule
Input & $64\times64\times1$ grayscale \\
Block 1 & $\{\text{Conv}(f_1,3\times3)\rightarrow \text{BN}\rightarrow \text{Swish}\}\times2$, MaxPool \\
Block 2 & $\{\text{Conv}(f_2,3\times3)\rightarrow \text{BN}\rightarrow \text{Swish}\}\times2$, MaxPool \\
Block 3 & $\{\text{Conv}(f_3,3\times3)\rightarrow \text{BN}\rightarrow \text{Swish}\}\times2$, MaxPool \\
Head & Flatten $\rightarrow$ Dense($d$) + Dropout $\rightarrow$ Dense(62) + Softmax \\
Regularization & L2 weight decay ($\lambda=10^{-4}$), progressive dropout \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Optimization Objective}
All deep models minimize categorical cross-entropy:
\[
\mathcal{L} = -\sum_{c=1}^{62} y_c \log p_\theta(y=c\mid x),
\]
optimized with Adam. Unless otherwise stated, model selection is based on validation accuracy, and final reporting is performed on the held-out test split.

\subsection{Hyperparameter Optimization and Training Schedule}
Hyperparameters (learning rate, dropout, L2 penalty, number of filters) are tuned via Bayesian search (e.g., Keras Tuner) on the validation split.
We adopt cosine learning-rate decay with warmup:
\begin{itemize}
  \item Warmup epochs: $E_w = 10$
  \item Total epochs: $E = 100$
  \item Early stopping: patience \texttt{<<<PATIENCE>>>} on validation loss (optional)
\end{itemize}

\begin{figure}[t]
  \centering
  \subfloat[Learning-rate schedule]{\includegraphics[width=0.48\linewidth]{lr_schedule.png}\label{fig:lr}}
  \hfill
  \subfloat[Loss/accuracy curves]{\includegraphics[width=0.48\linewidth]{loss_graph.png}\label{fig:loss}}
  \caption{Training dynamics for the optimized CNN: cosine decay with warmup and associated loss/accuracy trends.}
  \label{fig:training_dynamics}
\end{figure}

\subsection{Ensemble Inference with Test-Time Augmentation}
Single-model performance is improved via a soft-voting ensemble:
\[
p(y\mid x) = \sum_{m=1}^{M} w_m \, p_m(y\mid x),
\quad \sum_m w_m=1,
\]
where $M=3$ models and $w_m$ are nonnegative weights selected on the validation split.

\subsubsection{TTA Transform Set (Inference Only)}
For TTA, each test image is transformed $K=5$ times using mild geometric transforms consistent with training augmentation.
A typical configuration is:
\begin{itemize}
  \item Rotations: $\{-10^\circ, -5^\circ, 0^\circ, +5^\circ, +10^\circ\}$
  \item Translations: up to \SI{5}{\percent} (random within bounds) per sample
  \item Interpolation: bilinear; fill mode: constant (0)
\end{itemize}
Probabilities are averaged across transforms and models to reduce variance due to small orientation or alignment differences.

\subsubsection{Inference Procedure (Pseudo-code)}
\begin{figure}[t]
\centering
\begin{minipage}{0.95\linewidth}
\small
\begin{algorithmic}[1]
\STATE \textbf{Input:} image $x$, models $\{p_m\}_{m=1}^M$, weights $\{w_m\}$, TTA transforms $\{T_k\}_{k=1}^K$
\STATE \textbf{Output:} predicted label $\hat{y}$
\STATE $p \leftarrow \mathbf{0}$ \COMMENT{accumulator over classes}
\FOR{$k=1$ to $K$}
  \STATE $x_k \leftarrow T_k(x)$
  \STATE $p_k \leftarrow \mathbf{0}$
  \FOR{$m=1$ to $M$}
    \STATE $p_k \leftarrow p_k + w_m \cdot p_m(\cdot \mid x_k)$
  \ENDFOR
  \STATE $p \leftarrow p + p_k$
\ENDFOR
\STATE $p \leftarrow \frac{1}{K}p$
\STATE $\hat{y} \leftarrow \arg\max_c p_c$
\end{algorithmic}
\end{minipage}
\caption{Ensemble + TTA inference. TTA is applied only during inference; training uses separate on-the-fly augmentation.}
\label{fig:tta_algo}
\end{figure}

% -------------------- 5. Experimental Setup --------------------
\section{Experimental Setup}
\subsection{Compute Environment}
Classical models are trained on a local CPU (AMD Ryzen 5 7600).
Deep models are trained on GPU instances (e.g., NVIDIA T4/L4/A100).
Since hardware affects runtime but not final accuracy, we report accuracy-based comparisons under identical splits; runtime notes are included for completeness.

\subsection{Metrics}
We report:
\begin{itemize}
  \item \textbf{Accuracy} on the held-out test split
  \item \textbf{Macro-F1} to account for per-class behavior
  \item \textbf{Confusion matrix} and top confusion pairs
\end{itemize}
Macro-F1 is computed as the unweighted mean of per-class F1 scores.
When feasible, report mean$\pm$std across multiple seeds; if only a single seed is used, clearly state it (as done here).

\subsection{Implementation Notes (Reproducibility Checklist)}
To reduce ambiguity in reproduction, specify the following in the final submission:
\begin{itemize}
  \item Batch size, optimizer parameters, and weight decay
  \item Data loader preprocessing order (resize $\rightarrow$ normalize $\rightarrow$ augment)
  \item Model selection criterion (best validation accuracy vs. best validation loss)
  \item Exact TTA transform parameters and randomization
\end{itemize}

% -------------------- 6. Results --------------------
\section{Results}

\subsection{Classical Baseline Results}
Table~\ref{tab:xgboost} summarizes the classical pipeline performance.
The best HOG+XGBoost configuration reaches \SI{68.11}{\percent} accuracy, indicating a ceiling under this feature representation for fine-grained case-sensitive separation.

\begin{table}[t]
\centering
\caption{HOG+XGBoost Ablation Summary (Validation-Tuned, Test-Reported)}
\label{tab:xgboost}
\begin{tabular}{@{}p{0.62\linewidth}S[table-format=2.2]@{}}
\toprule
\textbf{Configuration} & {\textbf{Accuracy (\%)}} \\
\midrule
Baseline HOG ($8\times8$), default-ish XGBoost & 54.40 \\
Noise injection (strong) & 48.39 \\
Filtered ``easier'' subset (diagnostic) & 64.31 \\
\textbf{Final tuned configuration (HOG+XGBoost)} & \textbf{68.11} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Deep Learning and Ensemble Results}
Table~\ref{tab:dl_results} reports the progression from a basic CNN to an optimized single-model CNN and ensemble+TTA.
We avoid claims such as ``state-of-the-art'' unless validated against prior work under the same dataset and protocol; instead, we report results precisely for this benchmark and split.

\begin{table}[t]
\centering
\caption{Model Performance Progression (Fixed Split, Test Set)}
\label{tab:dl_results}
\begin{tabular}{@{}p{0.64\linewidth}S[table-format=2.2]@{}}
\toprule
\textbf{Model Strategy} & {\textbf{Accuracy (\%)}} \\
\midrule
Basic CNN (baseline) & 78.74 \\
Optimized VGG-style CNN (ReLU) & 88.48 \\
\textbf{Optimized VGG-style CNN (Swish + L2)} & \textbf{89.45} \\
\midrule
\textbf{Ensemble (soft voting) + TTA (K=5)} & \textbf{93.16} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Ablation on Training Choices (Deep Models)}
To attribute gains to specific design decisions, Table~\ref{tab:ablation_cnn} presents a controlled ablation (illustrative format; ensure values match your logged runs).
The intent is to separate architecture depth from optimization and regularization effects.

\begin{table}[t]
\centering
\caption{Deep Model Ablation (Illustrative Structure; Populate with Your Exact Runs)}
\label{tab:ablation_cnn}
\begin{tabular}{@{}p{0.52\linewidth}S[table-format=2.2]@{}}
\toprule
\textbf{Change (relative to baseline CNN)} & {\textbf{Accuracy (\%)}} \\
\midrule
Baseline CNN (ReLU, fixed LR, no L2) & 78.74 \\
+ VGG-style depth (more conv blocks) & 86.00 \\
+ Cosine decay with warmup & 87.50 \\
+ L2 weight decay ($10^{-4}$) & 88.70 \\
+ Swish activation (VGG + L2 + cosine) & 89.45 \\
\bottomrule
\end{tabular}
\end{table}

% -------------------- 7. Error Analysis --------------------
\section{Error Analysis}

\subsection{Case Sensitivity and Confusion Clusters}
The dominant failure mode is case ambiguity rather than structural mismatch.
Table~\ref{tab:f1_scores} contrasts representative case-sensitive confusions with structurally distinct characters.

\begin{table}[t]
\centering
\caption{Representative Per-Class Behavior (Illustrative Subset)}
\label{tab:f1_scores}
\begin{tabular}{@{}l S[table-format=1.2] S[table-format=1.2] S[table-format=1.2] @{\hspace{6mm}} l S[table-format=1.2] S[table-format=1.2] S[table-format=1.2] @{}}
\toprule
\multicolumn{4}{c}{\textbf{Case-Sensitive / Confusable}} & \multicolumn{4}{c}{\textbf{Structurally Distinct}} \\
\cmidrule(lr){1-4}\cmidrule(lr){5-8}
\textbf{Class} & {\textbf{Prec.}} & {\textbf{Rec.}} & {\textbf{F1}} &
\textbf{Class} & {\textbf{Prec.}} & {\textbf{Rec.}} & {\textbf{F1}} \\
\midrule
w (vs W) & 0.40 & 0.25 & 0.30 & 2 & 1.00 & 1.00 & 1.00 \\
0 (vs O) & 0.75 & 0.37 & 0.50 & b & 1.00 & 1.00 & 1.00 \\
l (vs I) & 0.66 & 0.50 & 0.57 & d & 1.00 & 1.00 & 1.00 \\
s (vs S) & 0.83 & 0.62 & 0.71 & i & 1.00 & 1.00 & 1.00 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Most Frequent Misclassifications}
Table~\ref{tab:top_errors} lists frequent label confusions, confirming that errors cluster around visually similar upper/lowercase or symbol/letter pairs.

\begin{table}[t]
\centering
\caption{Top Confusions on the Test Split}
\label{tab:top_errors}
\begin{tabular}{@{}l l S[table-format=1.0]@{}}
\toprule
\textbf{True} & \textbf{Predicted} & {\textbf{Count}} \\
\midrule
w & W & 6 \\
W & w & 3 \\
0 & O & 3 \\
v & V & 3 \\
s & S & 3 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Confusion Matrix (Placed in the Correct Context)}
Fig.~\ref{fig:confusion} visualizes the full confusion matrix.
The diagonal dominance indicates strong overall recognition, while specific off-diagonal clusters correspond to case-sensitive confusions (e.g., \textit{w/W}, \textit{s/S}) and symbol/letter ambiguity (e.g., \textit{0/O}).
This supports the claim that most errors are semantic (case/context) rather than gross shape failures.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{confusion_matrix.png}
  \caption{Confusion matrix highlighting concentrated off-diagonal clusters related to case sensitivity and symbol/letter ambiguity.}
  \label{fig:confusion}
\end{figure}

\subsection{Qualitative Error Gallery}
To complement the aggregate confusion view, Fig.~\ref{fig:error_gallery} shows representative misclassified samples.
These examples often appear visually plausible in isolation, reinforcing that a context-free classifier may be uncertain even when the learned representation is strong.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{error_gallery.png}
  \caption{Examples of misclassified characters illustrating high visual similarity (case or symbol/letter ambiguity).}
  \label{fig:error_gallery}
\end{figure}

% -------------------- 8. Discussion --------------------
\section{Discussion}
\subsection{Why the Classical Baseline Saturates}
HOG captures local gradient structure but can under-represent subtle cues required for reliable case discrimination in isolated characters.
Boosted trees can exploit structured features; however, if the feature space does not cleanly separate confusable classes, additional boosting yields diminishing returns.
This explains the observed plateau around \SI{68.11}{\percent} for the tuned HOG+XGBoost baseline.

\subsection{Why Ensemble+TTA Helps}
Ensembling reduces model variance and leverages complementary inductive biases (e.g., differences in activation functions, schedules, or training dynamics).
TTA further stabilizes predictions against nuisance transforms (slight rotation/translation), at the expense of inference cost proportional to $K$.
For practical deployment, the ensemble+TTA configuration is most appropriate when accuracy is prioritized over latency.

\subsection{Calibration and Decision-Making Under Ambiguity}
Case-sensitive and symbol/letter confusions indicate that some inputs are inherently ambiguous without context.
A pragmatic extension is to incorporate calibrated confidence and enable a \textit{reject option} (e.g., request user confirmation or defer to a downstream sequence model) when top-1 confidence is low.
Temperature scaling is a simple post-hoc calibration method often effective for neural classifiers \cite{guo2017calibration}.

\subsection{Limitations and Future Work}
Remaining errors suggest that purely visual, isolated-character recognition can be constrained by missing context.
Promising extensions include:
\begin{itemize}
  \item context-aware sequence modeling (e.g., character sequences with language priors),
  \item expanding the dataset, especially for confusable classes and diverse writing styles,
  \item reporting multi-seed mean$\pm$std and macro-F1 consistently for stronger statistical grounding,
  \item model compression or distillation to preserve accuracy while reducing ensemble inference cost.
\end{itemize}

% -------------------- 9. Conclusion --------------------
\section{Conclusion}
This paper presented a structured comparison of classical and deep learning approaches for 62-class handwritten character recognition under a fixed evaluation protocol.
HOG+XGBoost provides an efficient baseline (\SI{68.11}{\percent}), optimized CNNs substantially improve accuracy (\SI{89.45}{\percent}), and a multi-model ensemble with TTA further increases performance to \SI{93.16}{\percent}.
Error analysis indicates that the dominant residual failure mode is case sensitivity and visually confusable symbol/letter pairs, motivating context-aware approaches as a next step.

% -------------------- References --------------------
\begin{thebibliography}{00}

\bibitem{dalal2005hog}
N. Dalal and B. Triggs, ``Histograms of Oriented Gradients for Human Detection,'' in \emph{Proc. IEEE CVPR}, 2005.

\bibitem{chen2016xgboost}
T. Chen and C. Guestrin, ``XGBoost: A Scalable Tree Boosting System,'' in \emph{Proc. ACM KDD}, 2016.

\bibitem{simonyan2014vgg}
K. Simonyan and A. Zisserman, ``Very Deep Convolutional Networks for Large-Scale Image Recognition,'' arXiv:1409.1556, 2014.

\bibitem{ramachandran2017swish}
P. Ramachandran, B. Zoph, and Q. V. Le, ``Searching for Activation Functions,'' arXiv:1710.05941, 2017.

\bibitem{loshchilov2016sgdr}
I. Loshchilov and F. Hutter, ``SGDR: Stochastic Gradient Descent with Warm Restarts,'' arXiv:1608.03983, 2016.

\bibitem{guo2017calibration}
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, ``On Calibration of Modern Neural Networks,'' in \emph{Proc. ICML}, 2017.

\end{thebibliography}

\end{document}